"""
═══════════════════════════════════════════════════════════════════════════════
UNIFIED HIGH-PERFORMANCE PATHFINDER - PCB ROUTING ENGINE
═══════════════════════════════════════════════════════════════════════════════

ALGORITHM OVERVIEW:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

This implements the PathFinder negotiated congestion routing algorithm for
multi-layer PCB routing with GPU acceleration. PathFinder is an iterative
refinement algorithm that resolves resource conflicts through economic pressure.

CORE PATHFINDER LOOP (simplified):
───────────────────────────────────────────────────────────────────────────────
1. Initialize: Build 3D lattice graph with H/V layer constraints
2. For iteration = 1 to MAX_ITERATIONS:
     a) REFRESH: Load previous iteration's usage into present_usage arrays
     b) UPDATE_COSTS: Apply congestion penalties (present + historical)
     c) ROUTE: Route all nets using updated edge costs (Dijkstra/Delta-stepping)
     d) COMMIT: Save paths to edge_store for next iteration
     e) CHECK: If no overuse and all nets routed → SUCCESS, exit
     f) ESCALATE: Increase present_factor pressure for next iteration
3. If exited loop: analyze capacity and report failure

KEY INSIGHT: PathFinder uses economics - overused edges get expensive, forcing
nets to find alternative paths. Historical cost prevents oscillation.

═══════════════════════════════════════════════════════════════════════════════
DATA STRUCTURES & FLOW
═══════════════════════════════════════════════════════════════════════════════

GRAPH REPRESENTATION:
───────────────────────────────────────────────────────────────────────────────
• Lattice: 3D grid (x_steps × y_steps × layers) with H/V layer discipline
  - F.Cu (layer 0): Vertical routing (pad escapes only, short traces)
  - Inner layers: Alternating H/V polarity for Manhattan routing
  - Vias: Vertical edges between layers at same (x,y) location

• Storage: CSR (Compressed Sparse Row) format for GPU-efficient graph ops
  - indptr[i:i+1]: Edge range for node i
  - indices[j]: Destination node for edge j
  - weights[j]: Cost for edge j
  - Enables O(1) neighbor lookup, GPU-vectorized edge iteration

EDGE ACCOUNTING (Single Source of Truth):
───────────────────────────────────────────────────────────────────────────────
• _edge_store: Dict[int → int] - CSR edge index → usage count (canonical)
  - Persists between iterations
  - Updated by commit_net_path() when paths are routed

• edge_present_usage: Array[E] - Current iteration's usage (rebuilt each iter)
  - Refreshed from _edge_store at iteration start
  - Used for overuse calculation and cost updates

• edge_history: Array[E] - Accumulated historical congestion
  - Updated each iteration: hist[e] += gain * max(0, usage[e] - capacity[e])
  - Prevents nets from oscillating between same overused edges

• edge_total_cost: Array[E] - Final cost for routing (updated each iteration)
  - Formula: total_cost = base + pres_fac*overuse + hist_weight*history
  - This is what Dijkstra sees when finding shortest paths

COST EVOLUTION ACROSS ITERATIONS:
───────────────────────────────────────────────────────────────────────────────
Iteration 1: pres_fac=1.0  → Light penalties, nets find natural shortest paths
Iteration 2: pres_fac=1.6  → Moderate penalties on overused edges
Iteration 3: pres_fac=2.56 → Strong penalties, nets forced to find alternatives
...
Iteration N: pres_fac caps at 1000.0 → Extreme penalties, near-infinite cost

Historical cost accumulates: nets that repeatedly cause congestion get
permanently more expensive, preventing them from "hogging" good paths.

═══════════════════════════════════════════════════════════════════════════════
PATHFINDER NEGOTIATION LOOP - DETAILED WALKTHROUGH
═══════════════════════════════════════════════════════════════════════════════

ENTRY: route_multiple_nets(route_requests) → _pathfinder_negotiation()
───────────────────────────────────────────────────────────────────────────────

PHASE 1: INITIALIZATION
───────────────────────────────────────────────────────────────────────────────
• Parse net requests: (net_name, src_pad, dst_pad) → (net_id, src_idx, dst_idx)
• Build terminal lookup: Maps pad identifiers to lattice node indices
• Initialize edge arrays: present_usage=0, history=0, capacity=1 per edge
• Set pres_fac=1.0 (initial present factor)

PHASE 2: ITERATIVE NEGOTIATION (for iter = 1..MAX_ITERATIONS)
───────────────────────────────────────────────────────────────────────────────

STEP 2a: REFRESH USAGE FROM PREVIOUS ITERATION
  • _refresh_present_usage_from_store()
  • Rebuilds edge_present_usage from _edge_store (canonical usage)
  • Example: If _edge_store[42] = 3, then edge_present_usage[42] ← 3
  • This loads the "world state" from last iteration

STEP 2b: UPDATE EDGE COSTS WITH CONGESTION PENALTIES
  • _update_edge_total_costs(pres_fac)
  • For each edge e:
      overuse[e] = max(0, usage[e] - capacity[e])
      total_cost[e] = base_cost[e] + pres_fac*overuse[e] + hist_weight*history[e]
  • Overused edges become expensive → forces nets to avoid them
  • Transfer updated costs to GPU (if using GPU mode)

STEP 2c: BUILD HOTSET - IDENTIFY NETS THAT MUST RE-ROUTE
  • _build_hotset_from_overuse()
  • Find all edges where usage > capacity
  • Collect all nets that touch those edges → hotset
  • CRITICAL: Only hotset nets are re-routed this iteration (incremental mode)
  • Order hotset by congestion impact (most congested nets routed first)

STEP 2d: ROUTE NETS AGAINST UPDATED COSTS
  • GPU mode: _route_all_nets_gpu_in_batches_with_metrics()
    - Extracts ROI (Region of Interest) for each net: BFS from src/sink
    - Runs GPU Dijkstra (delta-stepping or near-far) on ROI subgraph
    - Returns paths as sequences of lattice node indices
  • CPU mode: _route_all_nets_cpu_in_batches_with_metrics()
    - Similar ROI extraction, CPU Dijkstra fallback

  • ROUTING ALGORITHM VARIANTS:
    - "near_far" (default): Fast ROI-based Dijkstra with bucket system
    - "delta_stepping": Parallel bucket-based shortest path (slower but robust)
    - "multi_roi": Batch multiple nets in parallel on GPU

  • OUTPUT: For each net, either a path (list of node indices) or None (failed)

STEP 2e: COMMIT PATHS TO EDGE STORE
  • _commit_present_usage_to_store()
  • For each net's path, increment usage count for all edges on that path
  • _edge_store[edge_idx] += 1 for each edge in path
  • This "locks in" the routing decisions for overuse calculation next iteration

STEP 2f: COMPUTE OVERUSE STATISTICS
  • _compute_overuse_stats_present()
  • For each edge: overuse = max(0, usage - capacity)
  • Returns: (overuse_sum, overuse_edges) = total overuse and # edges affected
  • Example: If 5 edges each have 2 nets (usage=2, capacity=1):
      overuse_sum = 5, overuse_edges = 5

STEP 2g: UPDATE HISTORICAL COST
  • _update_edge_history_from_present()
  • For each edge: history[e] += gain * overuse[e]
  • This "remembers" chronic congestion, preventing oscillation

STEP 2h: TERMINATION CHECK
  • SUCCESS: If overuse_sum == 0 AND failed_count == 0:
      → All nets routed with no conflicts → Return success, exit loop

  • STAGNATION: If overuse hasn't improved for N iterations:
      → Apply adaptive measures (widen ROI, increase pres_fac faster)
      → Eventually timeout after stagnation_patience iterations

  • CONTINUE: Otherwise, increase pres_fac and loop again
      pres_fac ← pres_fac * pres_fac_mult (typically 1.6)

PHASE 3: FAILURE ANALYSIS (if loop exits without convergence)
───────────────────────────────────────────────────────────────────────────────
• _finalize_insufficient_layers()
• Analyzes overuse to determine failure cause:
  - VIA-BOTTLENECK: >90% of overuse is on vias (adding layers won't help)
  - INSUFFICIENT-LAYERS: Routing congestion, estimate extra layers needed
• Aggregates overuse by spatial channel, computes percentile-based layer estimate
• Returns structured failure report with actionable recommendations

═══════════════════════════════════════════════════════════════════════════════
GPU ACCELERATION ARCHITECTURE
═══════════════════════════════════════════════════════════════════════════════

ROI EXTRACTION (Region of Interest):
───────────────────────────────────────────────────────────────────────────────
• Problem: Full graph is huge (10M+ nodes), but each net only uses small region
• Solution: Extract subgraph containing only nodes near src/sink
• Method: BFS expansion from src and sink up to margin distance
• Result: ROI typically 1000-10000 nodes vs 10M+ full graph (1000x smaller)

GPU DIJKSTRA VARIANTS:
───────────────────────────────────────────────────────────────────────────────
• Delta-stepping (_gpu_dijkstra_delta_stepping_csr):
  - Bucket-based parallel SSSP with "near" (≤δ) and "far" (>δ) edges
  - Processes all nodes in bucket simultaneously (massive parallelism)
  - Good for dense graphs with uniform edge weights

• Near-far worklist (_gpu_near_far_worklist_sssp):
  - Priority-based work queue with vectorized relaxation
  - Better for sparse graphs typical in PCB routing
  - Default mode for performance

COST SYNCHRONIZATION:
───────────────────────────────────────────────────────────────────────────────
• After each path commit, immediately update edge_total_cost on GPU
• Ensures subsequent nets in same batch see updated congestion
• Critical for incremental cost updates within iteration

═══════════════════════════════════════════════════════════════════════════════
GEOMETRY SYSTEM (KiCadGeometry)
═══════════════════════════════════════════════════════════════════════════════

LAYER DISCIPLINE:
───────────────────────────────────────────────────────────────────────────────
• H layers: Only horizontal edges allowed (y constant, x varies)
• V layers: Only vertical edges allowed (x constant, y varies)
• F.Cu (layer 0): Vertical, but short escape traces only (≤2 grid steps)
• Vias: Connect same (x,y) across layers, with legal layer-pair constraints

COORDINATE SYSTEMS:
───────────────────────────────────────────────────────────────────────────────
• World: (x_mm, y_mm, layer) - Physical PCB coordinates in millimeters
• Lattice: (x_idx, y_idx, layer) - Grid indices (0..x_steps, 0..y_steps)
• Node: flat_index - Single integer for CSR: layer*(x_steps*y_steps) + y*x_steps + x

CONVERSIONS:
• world_to_lattice(): (x_mm, y_mm) → (x_idx, y_idx) via grid snapping
• lattice_to_world(): (x_idx, y_idx) → (x_mm, y_mm) via pitch*idx + offset
• node_index(): (x_idx, y_idx, layer) → flat_index for CSR indexing
• index_to_coords(): flat_index → (x_idx, y_idx, layer)

═══════════════════════════════════════════════════════════════════════════════
CRITICAL INVARIANTS & ASSUMPTIONS
═══════════════════════════════════════════════════════════════════════════════

INVARIANT 1: Edge capacity = 1 per edge (no sharing)
• PathFinder assumes exclusive edge ownership
• Multiple nets on same edge = overuse = conflict

INVARIANT 2: _edge_store is canonical usage source
• Present usage arrays are rebuilt from _edge_store each iteration
• Never trust stale present_usage between iterations

INVARIANT 3: Costs must be updated before routing
• edge_total_cost MUST include current pres_fac penalties
• Stale costs → solver doesn't see congestion → no convergence

INVARIANT 4: H/V layer discipline must be enforced
• No horizontal edges on V layers, no vertical edges on H layers
• Violated → routing paths become illegal for PCB fabrication

INVARIANT 5: Node indices must be in range [0, node_count)
• Out-of-range indices → GPU crashes or silent corruption
• Validate all pad-to-lattice mappings at init time

═══════════════════════════════════════════════════════════════════════════════
COMMON FAILURE MODES & DEBUGGING
═══════════════════════════════════════════════════════════════════════════════

FAILURE: "No improvement for N iterations (stagnation)"
• CAUSE: Insufficient routing capacity, pres_fac not high enough
• FIX: Increase pres_fac_mult, add more layers, or relax design rules

FAILURE: "Routing succeeded but geometry emission failed"
• CAUSE: Paths not committed to routed_nets, or _negotiation_ran not set
• FIX: Ensure _commit_present_usage_to_store() completed successfully

FAILURE: "GPU routing takes 30+ seconds per net"
• CAUSE: ROI too large, extracting millions of edges
• FIX: Reduce ROI margin, use near_far mode, or fallback to CPU

FAILURE: "Path found but causes DRC violations"
• CAUSE: Base edge costs don't include clearance penalties
• FIX: Verify edge_base_cost includes track spacing and via clearances

FAILURE: "Via bottleneck - adding layers doesn't help"
• CAUSE: Via capacity exhausted (too many nets transitioning at same point)
• FIX: Use microvias, HDI stackup, or reduce via-to-via spacing

═══════════════════════════════════════════════════════════════════════════════
PERFORMANCE CHARACTERISTICS
═══════════════════════════════════════════════════════════════════════════════

TYPICAL PERFORMANCE (6-layer board, 1000 nets, 100×100mm):
• Lattice build: 2-5 seconds
• Per iteration: 5-15 seconds (GPU) or 30-60 seconds (CPU)
• Convergence: 3-8 iterations typical, up to 30 if congested
• Total time: 30-120 seconds for well-routed board

SCALING:
• Lattice size: O(x_steps × y_steps × layers) - grid resolution matters
• Routing time: O(iterations × nets × ROI_size × log(ROI_size))
• Memory: O(E) for CSR edges, typically 10-100M edges → 1-2GB GPU RAM

═══════════════════════════════════════════════════════════════════════════════
FILE ORGANIZATION
═══════════════════════════════════════════════════════════════════════════════

SECTIONS:
1. Configuration constants (lines ~65-140)
2. Data structures and utilities (lines ~140-650)
3. Initialization: __init__, initialize_graph, _build_3d_lattice (~1200-2600)
4. PathFinder core: _pathfinder_negotiation, cost updates (~2600-3700)
5. GPU routing: ROI extraction, Dijkstra variants (~3700-6700)
6. Geometry: emit_geometry, DRC validation (~9900-10500)
7. Helpers: coordinate transforms, layer analysis (~10500-11934)

KEY METHODS TO UNDERSTAND:
• initialize_graph(): Entry point, builds CSR graph from board
• _pathfinder_negotiation(): Main routing loop (THIS IS THE ALGORITHM)
• _update_edge_total_costs(): Applies PathFinder cost formula
• _gpu_roi_near_far_sssp_with_metrics(): GPU shortest path solver
• emit_geometry(): Converts paths to KiCad tracks/vias

═══════════════════════════════════════════════════════════════════════════════
"""
